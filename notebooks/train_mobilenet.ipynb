{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from os.path import join\n",
    "import tensorflow as tf\n",
    "\n",
    "from modules.datasets import ImageTargetDataset, RandomConcatDataset\n",
    "from modules.segm_transforms import train_transforms, test_transforms, ToTensorColor\n",
    "from modules.metrics import FbSegm\n",
    "from train.train import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "val_batch_size = 16\n",
    "INPUT_SIZE = (224, 224)\n",
    "AUG_PARAMS = [0.75, 1.25, 0.75, 1.25, 0.6, 1.4]\n",
    "ANG_RANGE = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trns = train_transforms(dataset='picsart', scale_size=INPUT_SIZE, ang_range=ANG_RANGE,\n",
    "                                      augment_params=AUG_PARAMS, add_background=False,\n",
    "                                      crop_scale=0.02)\n",
    "val_trns = test_transforms(dataset='picsart', scale_size=INPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs_hq = [\n",
    "    '/workdir/data/datasets/picsart/',\n",
    "    '/workdir/data/datasets/supervisely_person/',\n",
    "]\n",
    "\n",
    "data_dirs_coco = [\n",
    "    '/workdir/data/datasets/coco_person/'\n",
    "#     '/workdir/data/datasets/cityscapes_person/',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dirs_hq = [join(d, 'train') for d in data_dirs_hq]\n",
    "val_dirs_hq = [join(d, 'val') for d in data_dirs_hq]\n",
    "train_dirs_coco = [join(d, 'train') for d in data_dirs_coco]\n",
    "val_dirs_coco = [join(d, 'val') for d in data_dirs_coco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_hq = ImageTargetDataset(train_dirs_hq,\n",
    "                                           train_batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           device='GPU:0',\n",
    "                                           **train_trns,\n",
    "                                           IMG_EXTN='.jpg',\n",
    "                                           TRGT_EXTN='.png')\n",
    "val_dataset_hq = ImageTargetDataset(val_dirs_hq,\n",
    "                                           val_batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           device='GPU:0',\n",
    "                                           **val_trns,\n",
    "                                           IMG_EXTN='.jpg',\n",
    "                                           TRGT_EXTN='.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_coco = ImageTargetDataset(train_dirs_coco,\n",
    "                                           train_batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           device='GPU:0',\n",
    "                                           **train_trns,\n",
    "                                           IMG_EXTN='.jpg',\n",
    "                                           TRGT_EXTN='.png')\n",
    "val_dataset_coco = ImageTargetDataset(val_dirs_coco,\n",
    "                                           val_batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           device='GPU:0',\n",
    "                                           **val_trns,\n",
    "                                           IMG_EXTN='.jpg',\n",
    "                                           TRGT_EXTN='.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset_hq))\n",
    "print(len(train_dataset_coco))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RandomConcatDataset([train_dataset_hq, train_dataset_coco],\n",
    "                                    [0.8,0.2], size=3580)\n",
    "# train_dataset = train_dataset_hq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_dataset:\n",
    "    img, target = x[0], x[1]\n",
    "    for i in range(8):\n",
    "        print(img[i].shape)\n",
    "        print(target[i].shape)\n",
    "        plt.imshow(img[i])\n",
    "        plt.show()\n",
    "        plt.imshow(np.squeeze(target[i]))\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in val_dataset_hq:\n",
    "    img, target = x[0], x[1]\n",
    "    for i in range(8):\n",
    "        print(img[i].shape)\n",
    "        print(target[i].shape)\n",
    "        plt.imshow(img[i])\n",
    "        plt.show()\n",
    "        plt.imshow(np.squeeze(target[i]))\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model params\n",
    "device = 'GPU:0'\n",
    "model_name = 'mobilenet_small'\n",
    "# model_name = 'test_model'\n",
    "n_class=1\n",
    "# old_model_path = '/workdir/data/experiments/test/model_best_0.1734900027513504.h5'#32\n",
    "# old_model_path = '/workdir/data/experiments/fb_combined_mobilenetv3_tf/model_best_0.11067000031471252.h5'\n",
    "# old_model_path = '/workdir/data/experiments/fb_combined_mobilenetv3_tf_coco_pixart_supervisely/model_best_0.16210000216960907.h5'\n",
    "# old_model_path = '/workdir/data/experiments/fb_combined_mobilenetv3_tf_coco_pixart_supervisely/model_best_0.16161000728607178.h5'\n",
    "# old_model_path = '/workdir/data/experiments/fb_combined_mobilenetv3_tf_coco_pixart_supervisely/model_best_0.15041999518871307.h5'\n",
    "old_model_path = '/workdir/data/experiments/fb_combined_mobilenetv3_tf_coco_pixart_supervisely/model_best_0.14879000186920166.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train params\n",
    "n_train = len(train_dataset)\n",
    "n_val = len(val_dataset_hq)\n",
    "# loss_name='bce'\n",
    "loss_name = 'fb_combined'\n",
    "optimizer = 'Adam'\n",
    "lr = 0.00001\n",
    "batch_size = train_batch_size\n",
    "max_epoches = 1000\n",
    "save_directory = '/workdir/data/experiments/fb_combined_mobilenetv3_tf_coco_pixart_supervisely'\n",
    "reduce_factor = 0.75\n",
    "epoches_limit = 3\n",
    "early_stoping = 100\n",
    "metrics = [FbSegm(channel_axis=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model = Model(device=device,\n",
    "                        model_name=model_name,\n",
    "                        n_class=n_class,\n",
    "                        input_shape=(1,224,224,3),\n",
    "                        old_model_path=old_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model.prepare_train(train_loader=train_dataset,\n",
    "                              val_loader=val_dataset_hq,\n",
    "                              n_train=n_train,\n",
    "                              n_val=n_val,\n",
    "                              loss_name=loss_name,\n",
    "                              optimizer=optimizer,\n",
    "                              lr = lr,\n",
    "                              batch_size = batch_size,\n",
    "                              max_epoches = max_epoches,\n",
    "                              save_directory = save_directory,\n",
    "#                               use_bce=True,\n",
    "                              reduce_factor=reduce_factor,\n",
    "                              epoches_limit=epoches_limit,\n",
    "                              early_stoping=early_stoping,\n",
    "                              metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mobilenet_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model.validate(val_loader, n_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = os.listdir('/workdir/data/test_examples/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path in test_imgs:\n",
    "    img_path = os.path.join('/workdir/data/test_examples/', img_path)\n",
    "    test_img = cv2.imread(img_path)\n",
    "    test_img = test_img[:,:,::-1]\n",
    "    test_img = cv2.resize(test_img, (224,224))\n",
    "    test_tensor = ToTensorColor()(test_img)\n",
    "    test_tensor = tf.expand_dims(test_tensor, 0)\n",
    "    out = mobilenet_model.predict(test_tensor)\n",
    "    print(out.shape)\n",
    "    out_img = np.squeeze(out)#.numpy())\n",
    "    print(out_img.shape)\n",
    "    plt.imshow(test_img)\n",
    "    plt.imshow((out_img>0.5)*255, alpha=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert model to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(mobilenet_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"/workdir/data/converted_tflite_models/converted_model.tflite\",\n",
    "     \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(mobilenet_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_weights_quantized = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"/workdir/data/converted_tflite_models/converted_model_weights_quantized.tflite\",\n",
    "     \"wb\").write(tflite_model_weights_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(mobilenet_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset_generator():\n",
    "    for input_value, target_value in val_dataset_hq:\n",
    "            yield [input_value]\n",
    "#         yield [tf.expand_dims(input_value[0],0)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# representative_dataset_gen = val_dataset_hq\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset_generator = representative_dataset_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_full_quantized = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"/workdir/data/converted_tflite_models/converted_model_full_quantized.tflite\",\n",
    "     \"wb\").write(tflite_model_full_quantized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
